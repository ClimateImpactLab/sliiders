{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine data layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get elevation relative to sea level\n",
    "2. Match to nearest country, impact region, protection zone (e.g. levees)\n",
    "3. Uniformly distribute exposure over all surface area > 0 elevation within a 30\" pixel\n",
    "4. Aggregate both surface area and exposure up to adm1 X coastal segment X protection zone X wetland flag X .1-meter elevation bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from pathlib import Path\n",
    "import dask.distributed as dd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regionmask\n",
    "import rhg_compute_tools.gcs as rhgcs\n",
    "import rhg_compute_tools.kubernetes as rhgk\n",
    "import rhg_compute_tools.utils as rhgu\n",
    "import xarray as xr\n",
    "from shapely.geometry import box\n",
    "\n",
    "from sliiders import settings as sset\n",
    "from sliiders import spatial as spatial\n",
    "\n",
    "spatial.filter_spatial_warnings()\n",
    "\n",
    "\n",
    "@rhgu.block_globals\n",
    "def load_exposure(bbox, sset):\n",
    "    \"\"\"Get asset value and population within the bounds defined by `bbox`\"\"\"\n",
    "    llon, llat, ulon, ulat = bbox.bounds\n",
    "\n",
    "    # Get corners of `bbox` by their indices\n",
    "    lx_ix, ux_ix = spatial.grid_val_to_ix(\n",
    "        np.array([llon, ulon]),\n",
    "        sset.LITPOP_GRID_WIDTH,\n",
    "    )\n",
    "\n",
    "    ly_ix, uy_ix = spatial.grid_val_to_ix(\n",
    "        np.array([llat, ulat]),\n",
    "        sset.LITPOP_GRID_WIDTH,\n",
    "    )\n",
    "\n",
    "    # Define filters for reading parquet (saves computation and memory)\n",
    "    parquet_filters = [\n",
    "        [\n",
    "            (\"x_ix\", \">=\", lx_ix),\n",
    "            (\"x_ix\", \"<\", ux_ix),\n",
    "            (\"y_ix\", \">=\", ly_ix),\n",
    "            (\"y_ix\", \"<\", uy_ix),\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    exp_filters = [parquet_filters[0] + [(\"value\", \">\", 0)]]\n",
    "    pop_filters = [parquet_filters[0] + [(\"population\", \">\", 0)]]\n",
    "\n",
    "    # asset value\n",
    "    exp = pd.read_parquet(\n",
    "        sset.PATH_EXPOSURE_BLENDED,\n",
    "        columns=[\"value\", \"x_ix\", \"y_ix\"],\n",
    "        filters=exp_filters,\n",
    "    )\n",
    "\n",
    "    pop_landscan = pd.read_parquet(\n",
    "        sset.PATH_LANDSCAN_INT,\n",
    "        columns=[\"population\", \"x_ix\", \"y_ix\"],\n",
    "        filters=pop_filters,\n",
    "    ).rename(columns={\"population\": \"pop_landscan\"})\n",
    "\n",
    "    exp = pd.merge(\n",
    "        exp,\n",
    "        pop_landscan,\n",
    "        how=\"outer\",\n",
    "        left_on=[\"x_ix\", \"y_ix\"],\n",
    "        right_on=[\"x_ix\", \"y_ix\"],\n",
    "    )\n",
    "\n",
    "    exp[\"value\"] = exp[\"value\"].fillna(0)\n",
    "    exp[\"pop_landscan\"] = exp[\"pop_landscan\"].fillna(0)\n",
    "\n",
    "    return exp\n",
    "\n",
    "\n",
    "@rhgu.block_globals\n",
    "def get_protected_area_matches(elev_tile, bbox, sset):\n",
    "    \"\"\"\n",
    "    Get IDs of protected areas in `bbox`, returning a flattened array\n",
    "    corresponding to the flattened indices of `elev_tile`\n",
    "    \"\"\"\n",
    "    protected_areas = gpd.read_parquet(sset.PATH_GLOBAL_PROTECTED_AREAS)\n",
    "\n",
    "    return spatial.get_partial_covering_matches(\n",
    "        elev_tile, bbox, protected_areas, id_name=\"protection_zone_id\"\n",
    "    )\n",
    "\n",
    "\n",
    "@rhgu.block_globals\n",
    "def get_wetland_matches(elev_tile, bbox, sset):\n",
    "    \"\"\"\n",
    "    Get flag indicating existence of wetlands in `bbox`, returning a flattened array\n",
    "    corresponding to the flattened indices of `elev_tile`\n",
    "    \"\"\"\n",
    "    wetlands = gpd.read_file(sset.PATH_WETLANDS_INT, bbox=(bbox.bounds))\n",
    "\n",
    "    return spatial.get_partial_covering_matches(elev_tile, bbox, wetlands)\n",
    "\n",
    "\n",
    "@rhgu.block_globals\n",
    "def get_seg_adm(elev_tile, bbox, sset):\n",
    "    seg_adm = gpd.read_file(\n",
    "        sset.PATH_CIAM_ADM1_VORONOI_INTERSECTIONS_SHP,\n",
    "        bbox=box(*bbox.buffer(0.1).bounds),\n",
    "    )\n",
    "\n",
    "    return spatial.get_vor_matches(elev_tile, bbox, seg_adm, \"seg_adm\", \"seg_adm\")\n",
    "\n",
    "\n",
    "@rhgu.block_globals\n",
    "def match_elev_pixels_to_shapes(elev_tile, bbox, sset):\n",
    "\n",
    "    out_df = spatial.get_empty_exp_grid(elev_tile, sset.LITPOP_GRID_WIDTH)\n",
    "\n",
    "    out_df[\"seg_adm\"] = get_seg_adm(elev_tile, bbox, sset)\n",
    "    out_df[\"seg_adm\"] = out_df[\"seg_adm\"].astype(\"category\")\n",
    "\n",
    "    out_df[\"protection_zone\"] = get_protected_area_matches(elev_tile, bbox, sset)\n",
    "    out_df[\"protection_zone\"] = out_df[\"protection_zone\"].astype(\"category\")\n",
    "\n",
    "    out_df[\"wetland_flag\"] = get_wetland_matches(elev_tile, bbox, sset)\n",
    "    out_df[\"wetland_flag\"] = out_df[\"wetland_flag\"].astype(bool)\n",
    "\n",
    "    return out_df\n",
    "\n",
    "\n",
    "@rhgu.block_globals\n",
    "def get_valid_points_df(\n",
    "    elev_tile,\n",
    "    bbox,\n",
    "    all_points,\n",
    "    sset,\n",
    "):\n",
    "    elev_array = elev_tile.values.flatten()\n",
    "\n",
    "    all_points[\"z_ix\"] = spatial.grid_val_to_ix(\n",
    "        elev_array, sset.EXPOSURE_BIN_WIDTH_V, map_nans=0\n",
    "    )\n",
    "\n",
    "    all_points[\"valid\"] = (~np.isnan(elev_array)) & (\n",
    "        (all_points[\"z_ix\"] >= 0) | (all_points[\"protection_zone\"] != -1)\n",
    "    )\n",
    "\n",
    "    all_points[\"area_km\"] = spatial.get_cell_size_km(elev_tile, bbox)\n",
    "\n",
    "    out_types = {\n",
    "        \"x_ix\": np.int16,\n",
    "        \"y_ix\": np.int16,\n",
    "        \"z_ix\": np.int32,\n",
    "        \"seg_adm\": \"category\",\n",
    "        \"protection_zone\": \"category\",\n",
    "        \"wetland_flag\": bool,\n",
    "        \"area_km\": np.float32,\n",
    "    }\n",
    "\n",
    "    # compress\n",
    "    all_points = all_points.astype(\n",
    "        {k: v for k, v in out_types.items() if k in all_points.columns}\n",
    "    )\n",
    "\n",
    "    poselev_pts = (\n",
    "        all_points[all_points[\"valid\"]].drop(columns=[\"valid\"]).reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    negelev_pts = (\n",
    "        all_points[(~all_points[\"valid\"]) & (all_points[\"wetland_flag\"])]\n",
    "        .drop(columns=[\"valid\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return poselev_pts, negelev_pts\n",
    "\n",
    "\n",
    "@rhgu.block_globals\n",
    "def get_agg_fields():\n",
    "    \"\"\"Get fields to aggregate over\"\"\"\n",
    "    return [\n",
    "        \"z_ix\",\n",
    "        \"seg_adm\",\n",
    "        \"protection_zone\",\n",
    "    ]\n",
    "\n",
    "\n",
    "@rhgu.block_globals\n",
    "def write_empty_csv(out_path):\n",
    "    # write CSV placeholder to indicate this tile has been processed, but doesn't have exposure\n",
    "    pd.DataFrame().to_csv(out_path, index=False)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "@rhgu.block_globals\n",
    "def get_tile_out_path(tile_name, sset):\n",
    "    \"\"\"Get output path from the coastalDEM input path\"\"\"\n",
    "    return sset.DIR_EXPOSURE_BINNED_TMP_TILES / f\"{tile_name}.csv\"\n",
    "\n",
    "\n",
    "@rhgu.block_globals\n",
    "def get_exp_noland_out_path(tile_name, sset):\n",
    "    \"\"\"Get output path for exposure that couldn't be matched to land within its 1-degree elevation tile\"\"\"\n",
    "    return sset.DIR_EXPOSURE_BINNED_TMP_TILES_NOLAND / f\"{tile_name}.csv\"\n",
    "\n",
    "\n",
    "@rhgu.block_globals\n",
    "def get_seg_area_out_path(tile_name, sset):\n",
    "    \"\"\"Get output path for segment areas\"\"\"\n",
    "    return sset.DIR_EXPOSURE_BINNED_TMP_TILES_SEGMENT_AREA / f\"{tile_name}.csv\"\n",
    "\n",
    "\n",
    "@rhgu.block_globals\n",
    "def merge_exposure_to_highres_grid(this_exp, out, sset):\n",
    "    agg_fields = get_agg_fields()\n",
    "\n",
    "    ix_merge = pd.merge(\n",
    "        this_exp[[\"x_ix\", \"y_ix\"]],\n",
    "        out[[\"x_ix\", \"y_ix\", \"seg_adm\"]].drop_duplicates(),\n",
    "        left_on=[\"x_ix\", \"y_ix\"],\n",
    "        right_on=[\"x_ix\", \"y_ix\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    missing_exp_tiles = ix_merge[ix_merge[\"seg_adm\"].isnull()].drop(columns=[\"seg_adm\"])\n",
    "    valid_exp_tiles = ix_merge[ix_merge[\"seg_adm\"].notnull()].drop(columns=[\"seg_adm\"])\n",
    "\n",
    "    if valid_exp_tiles.shape[0] == 0:\n",
    "        valid_exp_tiles = out[[\"x_ix\", \"y_ix\"]].drop_duplicates()\n",
    "\n",
    "    missing_exp_tiles[\"lon\"] = spatial.grid_ix_to_val(\n",
    "        missing_exp_tiles[\"x_ix\"], sset.LITPOP_GRID_WIDTH\n",
    "    )\n",
    "    missing_exp_tiles[\"lat\"] = spatial.grid_ix_to_val(\n",
    "        missing_exp_tiles[\"y_ix\"], sset.LITPOP_GRID_WIDTH\n",
    "    )\n",
    "\n",
    "    valid_exp_tiles[\"lon\"] = spatial.grid_ix_to_val(\n",
    "        valid_exp_tiles[\"x_ix\"], sset.LITPOP_GRID_WIDTH\n",
    "    )\n",
    "    valid_exp_tiles[\"lat\"] = spatial.grid_ix_to_val(\n",
    "        valid_exp_tiles[\"y_ix\"], sset.LITPOP_GRID_WIDTH\n",
    "    )\n",
    "\n",
    "    exp_ix_mappings = (\n",
    "        spatial.get_closest_valid_exp_tiles(missing_exp_tiles, valid_exp_tiles)\n",
    "        if missing_exp_tiles.shape[0] > 0\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    if exp_ix_mappings is not None:\n",
    "        this_exp = pd.merge(\n",
    "            this_exp,\n",
    "            exp_ix_mappings,\n",
    "            left_on=[\"x_ix\", \"y_ix\"],\n",
    "            right_on=[\"x_ix\", \"y_ix\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "\n",
    "        this_exp[\"x_ix\"] = this_exp[\"valid_x_ix\"].fillna(this_exp[\"x_ix\"]).astype(int)\n",
    "        this_exp[\"y_ix\"] = this_exp[\"valid_y_ix\"].fillna(this_exp[\"y_ix\"]).astype(int)\n",
    "\n",
    "        this_exp = (\n",
    "            this_exp.groupby([\"x_ix\", \"y_ix\"])[[\"value\", \"pop_landscan\"]]\n",
    "            .sum()\n",
    "            .reset_index(drop=False)\n",
    "        )\n",
    "\n",
    "    exp_tile_areas = (\n",
    "        out.groupby([\"x_ix\", \"y_ix\"])[[\"area_km\"]]\n",
    "        .sum()\n",
    "        .rename(columns={\"area_km\": \"tile_area_km\"})\n",
    "    )\n",
    "\n",
    "    out = out.join(exp_tile_areas, on=[\"x_ix\", \"y_ix\"])\n",
    "\n",
    "    out = pd.merge(\n",
    "        out,\n",
    "        this_exp,\n",
    "        how=\"left\",\n",
    "        left_on=[\"x_ix\", \"y_ix\"],\n",
    "        right_on=[\"x_ix\", \"y_ix\"],\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    out = out.drop(columns=[\"x_ix\", \"y_ix\"])\n",
    "\n",
    "    out[\"value\"] = out[\"value\"] * out[\"area_km\"] / out[\"tile_area_km\"]\n",
    "    out[\"pop_landscan\"] = out[\"pop_landscan\"] * out[\"area_km\"] / out[\"tile_area_km\"]\n",
    "\n",
    "    out = out.drop(columns=[\"tile_area_km\"])\n",
    "\n",
    "    out[\"value\"] = out[\"value\"].fillna(0)\n",
    "    out[\"pop_landscan\"] = out[\"pop_landscan\"].fillna(0)\n",
    "\n",
    "    assert out.notnull().all().all()\n",
    "\n",
    "    out = out.drop(columns=[\"lon\", \"lat\"])\n",
    "\n",
    "    out = out.groupby(agg_fields, observed=True).sum().reset_index()\n",
    "\n",
    "    # make sure no exposure was dropped or added from the original exposure within tile (within some margin of float error)\n",
    "    # include very low sums for 0 / 0 division (areas where there is no exposure, but we calculate anyway for diva areas)\n",
    "    assert (\n",
    "        this_exp[\"value\"].sum() < 0.00001\n",
    "        or np.abs(this_exp[\"value\"].sum() / out[\"value\"].sum() - 1) < 0.00001\n",
    "    )\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "@rhgu.block_globals\n",
    "def process_tile(\n",
    "    tile_name,\n",
    "    sset,\n",
    "    calc_elev=True,\n",
    "    calc_exp=True,\n",
    "):\n",
    "    warnings.filterwarnings(\"ignore\", message=\"Geometry is in a geographic CRS\")\n",
    "    warnings.filterwarnings(\"ignore\", message=\"CRS mismatch between the CRS\")\n",
    "    warnings.filterwarnings(\n",
    "        \"ignore\", message=\"Sequential read of iterator was interrupted\"\n",
    "    )\n",
    "\n",
    "    out_path = get_tile_out_path(tile_name, sset)\n",
    "    bbox = spatial.get_bbox(tile_name)\n",
    "\n",
    "    this_exp = load_exposure(bbox, sset) if calc_exp else None\n",
    "\n",
    "    if calc_elev:\n",
    "        elev_tile = (\n",
    "            xr.open_rasterio(sset.DIR_MSS / f\"{tile_name}.tif\")\n",
    "            .squeeze(\"band\")\n",
    "            .drop(\"band\")\n",
    "        )\n",
    "        # Bundle higher-than-coastal elevation values into one to simplify later data processing\n",
    "        elev_tile = xr.where(elev_tile > sset.ELEV_CAP, sset.ELEV_CAP, elev_tile)\n",
    "    else:\n",
    "        elev_tile = spatial.get_granular_grid(bbox)\n",
    "\n",
    "    # match tile points with countries, impact regions, protection zones\n",
    "    out = match_elev_pixels_to_shapes(elev_tile, bbox, sset)\n",
    "\n",
    "    # get points on land, assign impact regions and countries at exposure grid level\n",
    "    out, negelev_pts = get_valid_points_df(elev_tile, bbox, out, sset)\n",
    "\n",
    "    # if calc_elev:\n",
    "    seg_areas = out.groupby(\n",
    "        [\"seg_adm\", \"protection_zone\", \"wetland_flag\", \"z_ix\"],\n",
    "        as_index=False,\n",
    "        observed=True,\n",
    "    )[\"area_km\"].sum()\n",
    "\n",
    "    negelev_areas = negelev_pts.groupby(\n",
    "        [\"seg_adm\", \"protection_zone\", \"wetland_flag\"],\n",
    "        as_index=False,\n",
    "        observed=True,\n",
    "    )[\"area_km\"].sum()\n",
    "    negelev_areas[\"z_ix\"] = -1\n",
    "\n",
    "    seg_areas = pd.concat([seg_areas, negelev_areas], ignore_index=True)\n",
    "\n",
    "    seg_areas = seg_areas[\n",
    "        (seg_areas[\"z_ix\"] <= 200) & (seg_areas[\"protection_zone\"] == -1)\n",
    "    ]\n",
    "\n",
    "    seg_out_path = get_seg_area_out_path(tile_name, sset)\n",
    "    seg_areas.to_csv(seg_out_path, index=False)\n",
    "    if not calc_exp:\n",
    "        return seg_out_path\n",
    "\n",
    "    if out.shape[0] == 0:\n",
    "        if calc_exp:\n",
    "            this_exp.to_csv(get_exp_noland_out_path(tile_name, sset), index=False)\n",
    "        return write_empty_csv(out_path)\n",
    "\n",
    "    out = (\n",
    "        out[~out[\"wetland_flag\"]].drop(columns=[\"wetland_flag\"]).reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    out = merge_exposure_to_highres_grid(this_exp, out, sset)\n",
    "\n",
    "    out.to_csv(out_path, index=False)\n",
    "\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy CIAM seg shapefiles if they haven't been updated for this version of the exposure grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maj_min(vers_name):\n",
    "    major, minor = vers_name.split(\".\")\n",
    "    return int(major), int(minor)\n",
    "\n",
    "\n",
    "exp_vers_maj, exp_vers_min = get_maj_min(sset.EXPOSURE_BINNED_VERS[1:])\n",
    "\n",
    "dir_shp = sset.DIR_CIAM_VORONOI.parent\n",
    "\n",
    "existing_vers = [get_maj_min(p.name[1:]) for p in list(dir_shp.glob(\"v*.*\"))]\n",
    "\n",
    "existing_vers.sort(key=lambda s: s[1])\n",
    "existing_vers.sort(key=lambda f: f[0])\n",
    "\n",
    "latest_vers_maj, latest_vers_min = existing_vers[-1]\n",
    "\n",
    "if (exp_vers_maj, exp_vers_min) not in existing_vers:\n",
    "\n",
    "    src_dir = dir_shp / (\"v\" + str(latest_vers_maj) + \".\" + str(latest_vers_min))\n",
    "    dst_dir = dir_shp / (\"v\" + str(exp_vers_maj) + \".\" + str(exp_vers_min))\n",
    "\n",
    "    rhgcs.cp(src_dir, dst_dir, flags=[\"r\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sset.DIR_EXPOSURE_BINNED.mkdir(exist_ok=True)\n",
    "\n",
    "sset.DIR_EXPOSURE_BINNED_TMP.mkdir(exist_ok=True)\n",
    "\n",
    "sset.DIR_EXPOSURE_BINNED_TMP_TILES.mkdir(exist_ok=True)\n",
    "sset.DIR_EXPOSURE_BINNED_TMP_TILES_NOLAND.mkdir(exist_ok=True)\n",
    "sset.DIR_EXPOSURE_BINNED_TMP_TILES_SEGMENT_AREA.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get list of tiles to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_meta_path = sset.PATH_EXPOSURE_TILE_LIST\n",
    "\n",
    "tile_meta = pd.read_parquet(tile_meta_path)\n",
    "\n",
    "tile_groups = tile_meta.groupby(\"PROCESSING_SET\")[\"tile_name\"].unique().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tiles = np.concatenate(list(tile_groups.values()))\n",
    "\n",
    "finished_tiles = [t[:-4][:7] for t in rhgcs.ls(sset.DIR_EXPOSURE_BINNED_TMP_TILES)]\n",
    "finished_segs = [\n",
    "    t[:-4][:7] for t in rhgcs.ls(sset.DIR_EXPOSURE_BINNED_TMP_TILES_SEGMENT_AREA)\n",
    "]\n",
    "\n",
    "remaining_tiles = [\n",
    "    t for t in all_tiles if (t not in finished_tiles and t not in finished_segs)\n",
    "]\n",
    "\n",
    "print(len(remaining_tiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client, cluster = rhgk.get_standard_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nworkers = 200\n",
    "cluster.scale(nworkers)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "from sliiders import __file__\n",
    "\n",
    "sliiders_dir = Path(__file__).parent\n",
    "zipf = zipfile.ZipFile(\"sliiders.zip\", \"w\", zipfile.ZIP_DEFLATED)\n",
    "for root, dirs, files in os.walk(sliiders_dir):\n",
    "    for file in files:\n",
    "        zipf.write(\n",
    "            os.path.join(root, file),\n",
    "            os.path.relpath(os.path.join(root, file), os.path.join(sliiders_dir, \"..\")),\n",
    "        )\n",
    "zipf.close()\n",
    "client.upload_file(\"sliiders.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Without elevation\n",
    "\n",
    "Note: when running the below three cells, one occasionally may run into the Dask cluster being stuck on making a progress. We find that this occurrence is not tile-specific. In such cases, we advise the user to follow these steps:\n",
    "\n",
    "\n",
    "1. Close the current Dask cluster and client by running `client.restart(); cluster.scale(0); client.close(); cluster.close()`\n",
    "2. Once the Dask cluster and client have successfully closed, restart the notebook kernel.\n",
    "3. Run all of the codes up to this section, and the cell directly below. Make sure that the Dask cluster is successfully running.\n",
    "4. Since we only need to remaining tiles that has not been processed, run (in via `client.map`) `process_tile` on these remaining ones. This can be done by running the below cells again, since the already-processed tiles would not be included in `remaining_tiles` anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withoutelev_tiles = np.array(\n",
    "    [t for t in tile_groups[\"WITHOUTELEV\"] if t in remaining_tiles]\n",
    ")\n",
    "\n",
    "withoutelev_tiles.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withoutelev_futures = client.map(\n",
    "    process_tile, withoutelev_tiles, sset=sset, calc_elev=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.progress(withoutelev_futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With elevation\n",
    "\n",
    "Note: similar to the without elevation cases, there could be cases in which Dask becomes stuck on making a progress. In such cases, we advise the user to follow similar steps to those explained above (but without having to re-run the steps involving without elevation workflow (i.e., after restarting the notebook, run all except the three cells under **Without elevation**, and work on the remaining tiles)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withelev_tiles = np.array([t for t in tile_groups[\"WITHELEV\"] if t in remaining_tiles])\n",
    "\n",
    "withelev_tiles.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withelev_futures = client.map(\n",
    "    process_tile,\n",
    "    withelev_tiles,\n",
    "    sset=sset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.progress(withelev_futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No exposure (CIAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciam_tiles = np.array([t for t in tile_groups[\"CIAM\"] if t in remaining_tiles])\n",
    "ciam_tiles.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciam_futures = client.map(\n",
    "    process_tile,\n",
    "    ciam_tiles,\n",
    "    sset=sset,\n",
    "    calc_exp=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.progress(ciam_futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shutdown workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
